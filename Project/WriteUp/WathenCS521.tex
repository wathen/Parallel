   % Journal Article Template
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\documentclass[10pt]{article}
% \documentclass{siamltex}
%
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{bm}
%\usepackage[notref,notcite]{showkeys}
%\usepackage[dvipdfm]{hyperref}
%\usepackage{hyperref}
\usepackage{graphicx}
%\usepackage{subfigure}
%\usepackage{epsfig,psfig}
\usepackage{amsfonts,amsmath,latexsym}
\usepackage{amsbsy}
\usepackage{nicefrac}
\usepackage{subcaption}
\usepackage{slashbox}
\usepackage{color}
\usepackage[usenames,dvipsnames]{xcolor}
\usepackage{sidecap}
\usepackage{calc}
\usepackage{enumitem}
\usepackage[hyphens]{url}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%s
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% PROOF
%
%\newenvironment{rproof}{\addvspace{\medskipamount}\par\noindent{\it Proof:\/}}
%{\unskip\nobreak\hfill$\Box$\par\addvspace{\medskipamount}}
%
% ROMENUM
%
\newcounter{bean}
\newenvironment{romenum}{\begin{list}{{(\roman{bean})}}
{\usecounter{bean}}}{\end{list}}
% \newtheorem{remark}[theorem]{Remark}
%
% special SYMBOLS
%
\newcommand{\uu}[1]{\boldsymbol #1}                     % vector fields
\newcommand{\uuu}[1]{\underline{#1}}                 % tensor fields
\newcommand{\jmp}[1]{[\![#1]\!]}                     % jump
\newcommand{\mvl}[1]{\{\!\!\{#1\}\!\!\}}             % mean value
%
% NORMS
%
\newcommand{\N}[1]{\|#1\|}                            % norm
\newcommand{\tn}[1]{|\!|\!|#1|\!|\!|}             % triple norm
%
\newcommand{\mc}[1]{\mathcal{#1}}
\newcommand{\curl}{\rm curl}
\newcommand{\dvr}{\rm div}
\newcommand{\nedelec}{N\'{e}d\'{e}lec }
\newcommand{\fenics}{{\tt FEniCS} }

\newcommand{\grad}{\ensuremath{\nabla}}
\newcommand{\RE}[1]{{\bf\textcolor{red}       {#1}}}
\newcommand{\re}[1]{{\textcolor{red}       {#1}}}
\newcommand{\orange}[1]{{\textcolor{Orange}       {#1}}}
\newcommand{\kns}{\mathcal{K}_{\rm NS}}
\newcommand{\km}{\mathcal{K}_{\rm M}}
\newcommand{\kc}{\mathcal{K}_{\rm C}}
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% end: our definitions
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%\newenvironment{Pf}{\noindent {\bf Proof:}} {\hfill $\Box$ \medskip}
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title{Parallel Finite Element assembly}
\author{
 Michael Wathen\thanks{Department of Computer Science,
The University of British Columbia, Vancouver, BC, V6T 1Z4, Canada,
 mwathen@cs.ubc.ca.}
}

\begin{document}

\maketitle

% \begin{abstract}

% \end{abstract}

% \begin{keywords}
% incompressible magnetohydrodynamics, saddle-point linear systems, null space, preconditioners, approximate inverse, Krylov subspace methods
% \end{keywords}

% {\small {\bf AMS Subject Classification.} 65F08, 65F10, 65F50, 65N22}
% 65F08: Preconditioners for iterative methods
% 65F10: Iterative methods for linear systems
% 65F15: Eigenvalues, eigenvectors
% 65N22: Solution of discretized equations
% 74S05: Finite element methods

\section{Introduction}

Many industrial and geophysical scientific computing problems require discrete solving techniques for partial differential equations (PDEs). The two key components of a PDE solving are:
\begin{itemize}
    \item Discretisation;
    \item Linear/non-linear solve.
\end{itemize}
For discretisation there are three main methods which are use finite differences, finite volumes and finite element methods (FEM). FEM discretises the model and represents the solution with respect to basis functions. Such methods rely heavily on variational methods from calculus of variations to approximate the solution.

The second key component is, the solving technique. Often, for large three-dimensional scientific computing problems it is not possible to do a direct solve for the system matrix. This is because such problems still have a large band width (lots of entries fair away from the diagonal), and hence the memory and time consumption is too large. Therefore, an iterative approach is required for these problems.

For a efficient FEM implementation the work to construct an $n\times n$ matrix is $\mathcal{O}(n)$. However, this is order for the solve is not necessarily true for all PDE models. For example, if the viscosity is small then it is known that for Navier-Stokes equations problem of solving the discrete system becomes harder. There is a huge amount of work that goes into the development for optimal ($\mathcal{O}(n)$) solvers for PDE; for example see \cite{bosch2014fast,elman2014finite,greif2007preconditioners}.

For this project we will mainly focus on parallel finite element method within the \fenics  software framework \cite{wells2012automated}. We will show assembly times for both the Laplacian and magnetohydrodynamics equations. Since the Laplacian plays an integral roll in many PDEs, we will also look at a the timings for a parallel solve of it.


\section{FEM in \fenics}

To understand the principle of parallel finite element methods, it is necessary to look at the sequential version. Given a certain cell, $\mathcal{T}_h$, with the local-to-global degrees of freedom mapping $i_T$ then the generic finite element assembly is given as:
\begin{algorithm}

FEM assembly
    \begin{algorithmic}[1]
    \State A = 0
    \For{$\mathcal{T} \in \mathcal{T}_h$}
        \State (1) Compute $i_T$
        \State (2) Compute $A_T$
        \State (3) Add $A_T$ to $A$ according to $i_T$:
        \For{$i \in \mathcal{I}_T$}
            \State $ A_{i_{T(i)}} \stackrel{+}{=} A_{T,i}$
        \EndFor
    \EndFor
    % \EndFor
    \end{algorithmic}
\end{algorithm}

From this algorithm, we see that the general principle of the FEM is to loop through the cells of the discretised domain and calculate the local cell matrices. Then the local elements are mapped back into the global matrix, $A$. This therefore means that for certain global degrees of freedom require multiple reads and writes.

The algorithm for a basic parallel finite element method does not really vary from the sequential version but assembles over a partitioned mesh. Therefore, for an efficient parallel implementation an appropriate mesh partitioning is required. The partitioned mesh should distribute the minimises the inter-process communication costs between the degrees of freedom.

In general, the ``natural'' ordering of the degrees of freedom within \fenics does....

 Two of the prominent mesh partitioning software packages are ParMETIS \cite{parmetis} and SCOTCH \cite{scotch}.

% Parallel finite element methods rely heavily on the

% Efficient parallel assembly relies on appropriately partitioned meshes and properly distributed degree-of-freedom maps to minimize inter-process communication. It is not generally possible to produce an effective degree-of-freedom map using only a form compiler, since the degree-of-freedom map should reflect the partitioning of the mesh. Instead, one may use a degree-of-freedom map generated by a form compiler to construct a suitable map at run-time. DOLFIN supports distributed meshes and computes distributed degree of freedom maps for distributed assembly.

% Multi-threaded2 assembly is outwardly simpler than distributed assembly and is attractive given the rapid growth in multi-core architectures. The assembly code can be easily modified, using for example OpenMP, to parallelize the assembly loop over cells. Multi-threaded assembly requires extra care so that multiple threads donâ€™t write to the same memory location (when multiple threads attempt to write to the same memory location, this is known as a race condition). Multi-threaded assembly has recently been implemented in DOLFIN (from version 1.0) based on coloring the cells of the mesh so that no two neighboring cells (cells that share a common vertex in the case of Lagrange elements) have the same color. One may then iterate over the colors of the mesh, and for each color use OpenMP to parallelize the assembly loop. This ensures that no two cells will read data from the same location (in the mesh), or write data to the same location (in the global tensor).


\section{Results}

In this section we will provide two examples. First, we consider a simple Laplacian where we look at both the assembly and solve times in parallel. Second, we look at the parallel assembly time for the magnetohydrodynamics problem.

The two numerical experiment have been carried out using the finite element software \fenics \cite{wells2012automated} together with {\tt PETSc4PY} package (Python interface for {\tt PETSc} \cite{petsc-user-ref,petsc-web-page}) and the multigrid package {\tt HYPRE} \cite{falgout2002hypre}.%The code is executed on a cluster with up to 12 nodes. Each node has eight 2.6 GHz Intel processors and 16 GB RAM.

\subsection{Laplacian}

For our first example, consider Laplace's equation with non-homogeneous Dirichlet boundary conditions.
\begin{equation} \label{eq:Lapl}
    \begin{array}{rcl}
        \Delta u &=& f \ \mbox{in} \ \Omega,\\
        u &=& g \ \mbox{on} \ \partial\Omega.
    \end{array}
\end{equation}
The Laplacian appears in fluid dynamics, electromagnetism and many other applications. Therefore, there has been a significant amount of work that has gone into efficient solvers for this system. It is well known that multigrid (either geometric (GM) or algebraic multigrid (AMG)) yields an optimal, $\mathcal{O}(n)$, solving algorithm.

The timing and solve results are shown in Tables~\ref{tab:Ltime}~and~\ref{tab:Lsolve}, respectively.
\begin{table}[h!]
    \centering
    \begin{tabular}{|c|ccccc|}
        \hline
        MPI & \multicolumn{5}{c|}{DoFs}\\
        processes &  14,739   &   107,811  &   823,875  &   6,440,067  & 50,923,779 \\
        \hline
            1  & 6.44e-01 &  2.69e+00 &  1.86e+01 &  1.49e+02 & - \\
            2  & 1.91e-01 &  1.38e+00 &  1.04e+01 &  7.92e+01 & - \\
            4  & 1.29e-01 &  1.05e+00 &  5.66e+00 &  4.19e+01 & - \\
            8  & 8.71e-02 &  5.31e-01 &  3.11e+00 &  2.32e+01 & 1.88e+02 \\
            16 & 1.16e-01 &  4.34e-01 &  2.12e+00 &  1.34e+01 & 9.94e+01 \\
            32 & 2.48e-01 &  3.17e-01 &  1.69e+00 &  1.20e+01 & 9.14e+01 \\
        \hline
    \end{tabular}
    \caption{Laplacian assembly time for different degrees of freedom and MPI processes}
    \label{tab:Ltime}
\end{table}
\begin{table}[h!]
    \centering
    \begin{tabular}{|c|ccccc|}
        \hline
        MPI & \multicolumn{5}{c|}{DoFs}\\
        processes &  14,739   &   107,811  &   823,875  &   6,440,067  & 50,923,779 \\
        \hline
        1  & 3.67e-01 &  4.50e+00 &  4.34e+01 &  3.93e+02 & - \\
        2  & 2.12e-01 &  2.90e+00 &  2.79e+01 &  1.92e+02 & - \\
        4  & 1.74e-01 &  1.46e+00 &  1.40e+01 &  1.21e+02 & - \\
        8  & 8.42e-02 &  1.18e+00 &  1.18e+01 &  9.24e+01 & 7.76e+02 \\
        16 & 8.28e-02 &  1.26e+00 &  9.11e+00 &  8.00e+01 & 6.71e+02 \\
        32 & 6.27e-02 &  9.35e-01 &  8.70e+00 &  7.66e+01 & 6.50e+02 \\
        \hline
    \end{tabular}
    \caption{Laplacian solve time for different degrees of freedom and MPI processes}
    \label{tab:Lsolve}
\end{table}
First, we look at the scalability with respect to the problem size. For the set up considered, the problems increase by a factor of 8 per level. From the tables, we observe that the that as we increase the degrees of freedom then both the assembly and solve time increase by roughly a factor of 8 for a fixed number of MPI processes. This factor is cleaner for the larger problems.

When varying the number of MPI processes the results don't scale quite as well, at least for the solve time. From Table~\ref{tab:Ltime} the times do seem to decrease by a factor of 2 when the number of MPI processes are increased from 1 to 16. However, moving 32 processes does not decrease the time by a factor of 2 from 16. The solve times do not exhibit this behaviour, though the timings do decrease as the number of MPI processes increase.

\subsection{MHD} \label{sec:MHD}

For our second example, we will consider an incompressible magnetohydrodynamics model with Dirichlet boundary conditions:
\begin{subequations}
\label{eq:mhd}
\begin{alignat}2
\label{eq:mhd1} - \nu  \, \Delta\uu{u} + (\uu{u} \cdot \nabla)
\uu{u}+\nabla p - \kappa\,
(\nabla\times\uu{b})\times\uu{b} &= \uu{f} & \qquad &\mbox{in $\Omega$},\\[.1cm]
\label{eq:mhd2}
\nabla\cdot\uu{u} &= 0 & \qquad &\mbox{in $\Omega$},\\[.1cm]
\label{eq:mhd3}
\kappa\nu_m  \, \nabla\times( \nabla\times \uu{b})
+ \nabla r
- \kappa \, \nabla\times(\uu{u}\times \uu{b}) &= \uu{g} & \qquad &\mbox{in $\Omega$},\\[.1cm]
\label{eq:mhd4} \nabla\cdot\uu{b} &= 0 & \qquad &\mbox{in
$\Omega$}.
\end{alignat}
\end{subequations}

In a standard FEM fashion, we linearize around the current velocity and magnetic fields and introduce basis functions corresponding to the discrete spaces as in \cite{schotzau2004mixed}. This yields the following matrix system:
\begin{equation}
\label{eq:mhd_saddle}
%\mathcal{K} x \equiv
\left(
\begin{array}{cccc}
F(u) & B^T & C(b)^T & 0\\
B & 0 & 0 & 0 \\
-C(b) & 0 & M & D^T\\
0 & 0 & D & 0
\end{array}
\right)
\,
\left(
\begin{array}{c}
\delta u\\
\delta p\\
\delta b\\
\delta r
\end{array}
\right)  =
\begin{pmatrix}
r_u \\
r_p\\
r_b\\
r_r
\end{pmatrix},
\end{equation}
with
\begin{equation} \nonumber
\begin{array}{rl}
r_u &= f- F(u) u - C(b)^T b- B^T p,\\
r_p &=-B u,\\
r_b &=g-Mu+C(b)b-D^T r,\\
r_r &=-D b,
\end{array}
\end{equation}
where $F(u) = A+O(u)$. The matrices are: ${F}$, the discrete convection-diffusion operator; ${B}$, a discrete divergence operator; ${M}$, the discrete curl-curl operator; ${D}$, a discrete divergence operator; and ${C}$, a discrete coupling term. For full discretisation details see \cite[Chapter 2]{mythesis}.

At this point there is no known parallel implementation of this discretisation for the MHD model with these specific elements used in \cite{schotzau2004mixed}. In the recent paper \cite{phillips2014block}, the authors look at an exact penalty form of the MHD model. This discretisation yields a $3\times3$ linear system to be solved at each non-linear iteration. Their implementation appears to be the first parallel approach for of both the solve and finite element discretisation.

The finite element assembly results are presented in Table~\ref{tab:MHD}.
\begin{table}[h!]
    \centering
    \begin{tabular}{|c|ccccc|}
        \hline
        MPI & \multicolumn{5}{c|}{DoFs}\\
        processes &  20,381   &   148,661  &   1,134,437  &   8,861,381  & 70,045,061 \\
        \hline
        1 & 4.82e+00 &  3.94e+01 &  3.04e+02 &  2.46e+03 &  - \\
        2 & 3.08e+00 &  2.09e+01 &  1.58e+02 &  1.30e+03 &  - \\
        4 & 1.50e+00 &  1.06e+01 &  8.49e+01 &  6.60e+02 &  - \\
        8 & 7.88e-01 &  5.86e+00 &  4.65e+01 &  3.58e+02 &  2.81e+03 \\
        16 & 6.72e-01 &  3.33e+00 &  2.40e+01 &  1.88e+02 &  1.47e+03 \\
        32 & 6.66e-01 &  3.29e+00 &  2.35e+01 &  1.85e+02 &  2.01e+03 \\
        \hline
    \end{tabular}
    \caption{MHD assembly time for different degrees of freedom and MPI processes}
    \label{tab:MHD}
\end{table}
Due to the much greater complexity of the MHD model \eqref{eq:mhd} than the Laplacian \eqref{eq:Lapl}, we would expect that the system assembly times to be generally higher for a similar number problem size. This is exactly what we observe between the Tables~\ref{tab:MHD}~and~\ref{tab:Ltime}. As with the Laplacian assembly results the MHD model does seem to scale with the problem size. Again, we see a similar scaling behaviour with respect to the number of MPI processes until we reach 32 MPI threads. We see that the computation time stalls or possible increases from 16 to 32 processes.

\section{Conclusion}

We have looked at a parallel finite element discretisation of the well-known Laplacian as well as new MPI implementation of the MHD model described in \cite{schotzau2004mixed}. For our numerical experiments we have used the comprehensive \fenics finite element software package.

The Laplacian is a well understood problem, but it forms an integral part of many PDEs. Therefore, it is a good starting point for numerical test. We saw that the parallel assembly times for the Laplacian exhibit the optimal ($\mathcal{O}(n)$) times across specified MPI processors. This is particularly important for the larger three-dimensional meshes. We also presented parallel solve times for the Laplacian. Again, the solver exhibited optimal, $\mathcal{O}(n)$, times with respect to the mesh.

Finally, we looked at the move complex MHD model. MHD models electrically conductive fluids in the presents of a electromagnetic field. So far there are very few parallel finite element implementations of these models. The results shown in Section~\ref{sec:MHD} show good scalability with respect to the mesh. We see that it takes roughly the same time to construct a matrix of dimension 8,861,381 on one processor as it does to construct a problem of 70,045,061 on eight processor. This is only the starting point of parallel MHD. The aim is to implement a parallel solve using block based preconditioners similar to \cite{phillips2014block,mythesis} to create a fully parallel and scalable solution method.

% We have looked at a finite element  new block preconditioning techniques for the MHD model \eqref{eq:mhd}--\eqref{eq:bc}. Our aim was to develop indefinite preconditioning approaches that utilizes the Maxwell maximal nullity result in \cite{MaximalNull} and adapt it to the MHD model.

% Using the results in \cite{MaximalNull}, we are able to find exact expressions for both a block Schur complement of the MHD model and its inverse. Using this Schur complement we presented a block triangular preconditioner as well as an approximate inverse preconditioner.

% The preliminary numerical experiments demonstrate the viability and effectiveness of our approach. Plans for future work include further optimizing our code, utilizing inexact solves to reduce the overall computational work, and applying our preconditioners to three-dimensional problems.

% The timing results for the approximate inverse preconditioner are at maximum a factor of two more than the block triangular preconditioner. When considering inexact (AMG, etc.) solves on the preconditioners it could be possible to loosen the solve tolerances for the approximate inverse, more than the block triangular approach which would decrease this time factor. We leave this as an area for future development.

% For the class of approximate inverse preconditioners, it is possible to obtain the nice property that the real part of the eigenvalues for the preconditioned matrix are all positive, in contrast to the case of typical block diagonal/triangular preconditioners. If the preconditioned matrix were symmetrizable by a similarity transformation, then this positivity property may be exploitable by using variations of positive definite solvers. This is left as an area for future work.

% In conculio


\bibliographystyle{plain}
\bibliography{ref}

\end{document}
